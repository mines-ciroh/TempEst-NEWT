{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0413d1a9-4f94-44d5-af96-68e98e8c153f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# TempEst-NEXT Validation Suite\n",
    "\n",
    "This Notebook provides a standard suite of model tests for TempEst-NEXT.  This serves two purposes:\n",
    "\n",
    "1. Reproducibility of research.  This notebook is used to generate final manuscript figures relating to model performance. The notebook itself is provided with the published model and dataset, allowing exact reproducibility (and easy modification) of the analysis.\n",
    "2. Efficient, consistent testing.  After modifying the model, running this notebook is a quick way to make sure everything still works and to assess performance impacts.\n",
    "\n",
    "For validation, we use two pre-retrieved datasets as well as some automatic retrieval of new data.  The two pre-retrieved datasets are a \"development\" set of ~900 USGS gages (nominally 1,000; 900ish with overlapping data coverage) and a \"test\" set of 331 (nominal 400) USGS gages paired with daymet meteorology, 3DEP topography, NLCD land cover, etc.  The development set was used for model development and tuning, while the test set is reserved for final validation (i.e., here).  Test-set validations have been run during the development process, but are never used to directly inform model design.\n",
    "\n",
    "The test suite also illustrates model runtime.\n",
    "\n",
    "Some cells were used for initial data preparation, but do not need to be rerun.  These have been set to \"raw\" rather than Python, so simply running the entire Notebook will run all tests but nothing extraneous.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To run the entire Notebook, requirements are:\n",
    "\n",
    "- Python dependencies: TempEst-NEXT, matplotlib, and seaborn.  TempEst-NEXT is available (under that name) on the Python Package Index, so `pip install TempEst-NEXT`.  This will also install, as a dependency, TempEst-NEWT, the calibrated version of the model.\n",
    "- A directory (specify location in `bp=` in the first cell) containing `DevData.csv`, `TestData.csv`, `DevDataHRRR3.csv`, and `TestDataHRRR3.csv`.  These should be provided with the notebook.  It should also contain a `results` subdirectory.\n",
    "- In the same directory as the notebook:\n",
    "    - An `ecoregions` directory, containing `NA_CEC_Eco_Level1.shp` (EPA Level I Ecoregions).\n",
    "    - A `val_figures` directory\n",
    "\n",
    "The preprocessing chunks have additional requirements, but are only used to generate the above data files and so don't need to be run.  They are set to `raw`, not `code`, so if the entire notebook is run they will not be executed.\n",
    "\n",
    "Running the Notebook will reproduce all manuscript performance data and figures, except those relating to \"true\" forecasting (as opposed to reforecasting).  This is because performance evaluation for \"true\" forecasting requires separate steps to be run days apart and therefore can't be fully automated in a single run, though simply generating the forecast is automatic.\n",
    "\n",
    "## Assessed Model Characteristics\n",
    "\n",
    "The goal is to assess several model performance characteristics.  Forecasting is used for a handful of tests, but most analysis focuses on hindcasting for computational convenience.  It is assumed that any performance discrepancies in forecasting vs hindcasting would be apparent in the tests that cover both, and thus that not every analysis needs to test forecasting.\n",
    "\n",
    "In this notebook, forecasting is primarily tested by using archived weather forecasts to predict what the forecast would have been for time periods where observations are available, which is necessary for automatic testing.  There is code to run a \"real\" forecast, but this cannot be automatically evaluated because observations are not available, so the user is left to go back and check once observations are available.\n",
    "\n",
    "1. Calibrated model hindcasting and forecasting performance, using TempEst-NEWT like a typical single-watershed model\n",
    "2. General ungaged hindcasting and forecasting performance\n",
    "3. Ungaged-region hindcasting performance\n",
    "4. Ungaged-elevation hindcasting performance\n",
    "5. Ungaged-time-period hindcasting performance\n",
    "6. Disturbance hindcasting performance?  TBD\n",
    "7. Small-stream hindcasting performance?  TBD\n",
    "\n",
    "## Tests\n",
    "\n",
    "The following tests are used to assess the above performance characteristics.  Forecast tests cover just 2022, since HRRR retrieval is slow.\n",
    "\n",
    "- Calibrated testing: train a model on the first 70% of observations for each stream, then evaluate performance for predicting the last 30%.  This uses the full dataset.\n",
    "  - Use meteorology estimates (daymet) for training and testing: hindcast test.  Because the model architecture does not actually use \"today's\" weather (up through yesterday only), this is also a 24-hour forecast test.\n",
    "  - Use weather forecast archives (HRRR, GFS/GEFS) for training and testing: forecast test.  Test forecast period of 2 days.\n",
    "- Gagewise cross-validation: partition development dataset gages into *k* equal sets.  Train a model on all partitions but one, and evaluate performance for predicting the excluded partition.  This tests general ungaged performance, not accounting for any potential impact of having used the same dataset for model tuning.  Hindcasting (met estimate) only.  This uses the development dataset to show whether model tuning introduced any performance discrepancies (compared to test set validation).\n",
    "- Test set validation: train a model on the development set, and evaluate performance for predicting the test set.  This tests general ungaged performance for a fully-independent dataset.\n",
    "  - Meteorology estimates (hindcast)\n",
    "  - Weather forecast archives (forecast) for 2-day (tomorrow) forecasts.  This has to be done with HRRR, since GFS doesn't have as readily-available long-term archives.\n",
    "    - Trained on estimates/test on forecasts (forecast with heterogeneous data).\n",
    "    - Trained on forecasts (homogeneous).\n",
    "- Extrapolation hindcasting tests: partition the combined development and testing sets along some characteristic of interest, and use a model trained on one group to predict the other group.  This tests the ability of TempEst-NEXT to extrapolate in terms of specific characteristics.  All hindcasting.\n",
    "  - Regional: partition the CONUS into contiguous regions and run leave-one-out cross-validation over the regions.\n",
    "  - Elevation: train a model on the lower elevations and predict higher elevations.  Partial dependency plots and previous research suggest there is a major shift in watershed dynamics around 2300 m, and it is difficult to extrapolate past that barrier.\n",
    "  - Time (walk-forward validation): train a model up to a given year, then predict the next year.  This tests whether the model can extrapolate forward in time.\n",
    "- Regime-shift hindcasting: identify a set of watersheds for which the observed thermal regime has shifted significantly.  Train the model on everything else, then try to predict the disturbed watersheds and see how the model performs.  This assesses whether TempEst-NEXT is capable of capturing regime shifts.\n",
    "- Small-stream hindcasting: if possible, use the model to predict temperatures at very small (e.g., first-order headwaters, centimeters to a few meters wide) streams where some observations are available, just to see if it works there.\n",
    "- True forecasting: run TempEst-NEXT with a 1-16 day lead time in real time (HRRR and GFS), then come back and check the performance.  This needs to be rerun many times, preferably over multiple seasons, to be meaningful.\n",
    "\n",
    "\n",
    "A full run takes about 3 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e470d85-7d95-49d5-b836-5f8bb2d9a8bf",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac96a84d-dbd2-4ff2-909f-7b404cdc4efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: Open of /u/wy/ch/dphilippus/.conda/envs/next/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "import NEXT\n",
    "from NEXT import wforecast\n",
    "import NEWT\n",
    "from rtseason import ThreeSine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import dataretrieval.nwis as nwis\n",
    "import geopandas as gpd\n",
    "import matplotlib.animation as anim\n",
    "import shapely\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scores.probability import brier_score_for_ensemble\n",
    "import xarray as xr\n",
    "from math import floor, ceil\n",
    "warnings.catch_warnings(action=\"ignore\")\n",
    "sns.set_context(\"paper\")\n",
    "bp = \"/scratch/dphilippus/notebooks/next_validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadbace-7143-40af-afac-4a7957f31d5e",
   "metadata": {},
   "source": [
    "Use the `variant` options below to indicate the use of a different model implementation, so that all validations are rerun and results saved separately.  `coef_variant` refers to coefficient estimation and `newt_variant` to NEWT model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75380878-45b1-4532-a670-5d4b0197e813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coef_variant = \"_rv2pca\" # current reference: FWPCA, 2PC\n",
    "newt_variant = \"_reference\"\n",
    "rerun = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d636d2-4aaa-4205-bb94-0a5b5f8ef31e",
   "metadata": {},
   "source": [
    "There are some major outliers that are either erroneous (negative temperatures) or wildly unrepresentative (hot springs) that we remove to produce realistic performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e198159-0acb-4c73-bf13-b7895e418b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datafiles = [bp + x + \".csv\" for x in [\"DevDataBuffers\", \"TestDataBuffers\", \"DevDataHRRR3\", \"TestDataHRRR3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286c37a-32ea-4718-9bf5-040a10c46d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv(bp + \"DevDataBuffers.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"]).drop(columns=[\"swe\", \"tmin\"]).dropna()\n",
    "dev_data = dev_data[(dev_data[\"temperature\"] > -0.5) & (dev_data[\"temperature\"] < 40)]\n",
    "test_data = pd.read_csv(bp + \"TestDataBuffers.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"]).dropna()\n",
    "test_data[\"day\"] = test_data[\"date\"].dt.day_of_year\n",
    "all_data = pd.concat([dev_data, test_data]).drop(columns=[\"asp_north\", \"asp_east\", \"id_type\", \"unknown\", \"elev_std\", \"frozen\", \"cold_prcp\"])\n",
    "test_data_hrrr = pd.read_csv(bp + \"TestDataHRRR3.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"]).merge(test_data[[\"id\", \"date\", \"canopy\", \"flowdir\"]], on=[\"id\", \"date\"], how=\"left\")\n",
    "dev_data_hrrr = pd.read_csv(bp + \"DevDataHRRR3.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"]).merge(dev_data[[\"id\", \"date\", \"canopy\", \"flowdir\"]], on=[\"id\", \"date\"], how=\"left\")\n",
    "all_data_hrrr = pd.concat([test_data_hrrr, dev_data_hrrr]).drop(columns=[\"elev_std\", \"asp_north\", \"asp_east\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c638b-9bf5-4fd5-984e-a605f36ffb3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Forecast Archive Retrieval (does not need to be run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9346f-1f30-47aa-9c08-05350384a1ab",
   "metadata": {},
   "source": [
    "A one-month run takes 0.02 hours, so 72 months should be about 1.5 hours, for a total of ~500 core-hours for the test set.  Needs to be run distributed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "38edd9d9-a09a-4d3e-bba5-a73414bc114d",
   "metadata": {
    "tags": []
   },
   "source": [
    "sites = all_data[\"id\"].unique()\n",
    "start = time.time()\n",
    "for (i, site) in enumerate(sites):\n",
    "    (ws, lat, lon, area) = NEXT.data.gage_geom(site)\n",
    "    hrrr = NEXT.data.weather_hrrr(ws, \"2022-12-01\", \"2022-12-31\")\n",
    "    hrrr.to_csv(bp + f\"hrrr/{site}.csv\", index_label=\"date\")\n",
    "    runtime = (time.time() - start) / 3600\n",
    "    print(f\"\\rRan {i:03d} sites in {runtime:.03f} hours\", end=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b10f5b8-a6a5-444c-9b0c-832929b22cad",
   "metadata": {
    "tags": []
   },
   "source": [
    "hrrr_files = os.listdir(bp + \"hrrr/\")\n",
    "hrrr = pd.concat([pd.read_csv(bp + \"hrrr/\" + f, parse_dates=[\"date\"]).assign(id=f.split(\".\")[0]) for f in hrrr_files])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fd554b4-c7eb-4874-b30e-4850c5e6c1e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_data_hrrr = test_data.drop(columns={\"tmax\", \"prcp\"}).merge(hrrr, on=[\"id\", \"date\"], how=\"inner\")\n",
    "test_data_hrrr.to_csv(bp + \"TestDataHRRR3.csv\", index=False)\n",
    "dev_data_hrrr = dev_data.drop(columns={\"tmax\", \"prcp\"}).merge(hrrr, on=[\"id\", \"date\"], how=\"inner\")\n",
    "dev_data_hrrr.to_csv(bp + \"DevDataHRRR3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abec64-e1fc-47e6-9f18-422f0f39e9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Preprocessing (does not need to be run)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8b57b0c-315c-4112-81e9-1949a0efef93",
   "metadata": {},
   "source": [
    "# dev_data[\"day\"] = dev_data[\"date\"].dt.day_of_year\n",
    "# gsamp = pd.read_csv(\"GageSample.csv\",\n",
    "#                    dtype={\"id\": \"str\"})\n",
    "# def idfix(data):\n",
    "#     data = data[data[\"id\"].apply(lambda x: x.startswith(\"USGS\"))]\n",
    "#     data[\"id\"] = data[\"id\"].apply(lambda x: x.split(\"_\")[1])\n",
    "#     return data\n",
    "# lcov = idfix(pd.read_csv(\"LandCover.csv\"))\n",
    "# area = idfix(pd.read_csv(\"Area.csv\"))\n",
    "# topo = idfix(pd.read_csv(\"Topography.csv\"))\n",
    "# dev_data = dev_data.merge(lcov, on=\"id\").merge(area, on=\"id\").merge(topo, on=\"id\").merge(gsamp[[\"id\", \"lat\", \"lon\"]], on=\"id\")\n",
    "# dev_data.to_csv(bp + \"DevData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e0ff40e-c006-442e-89a4-23543fe63834",
   "metadata": {
    "tags": []
   },
   "source": [
    "dev_ids = dev_data[\"id\"].unique()\n",
    "del dev_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ff91673-cfe1-4fd4-bd8a-97838ac68e23",
   "metadata": {
    "tags": []
   },
   "source": [
    "test_path = bp + \"TestData.csv\"\n",
    "def get_weather(site):\n",
    "    path = f\"/scratch/dphilippus/pyproc/daymet_aoi/USGS-{site}.csv\"\n",
    "    return pd.read_csv(path, parse_dates=[\"datetime\"])[[\"id\", \"datetime\",\n",
    "                                                            \"variable\",\n",
    "                                                            \"mean\"]].\\\n",
    "        pivot(index=[\"id\", \"datetime\"], columns=\"variable\", values=\"mean\").\\\n",
    "            reset_index().\\\n",
    "            rename(columns={\"datetime\": \"date\"})[[\"date\", \"tmax\", \"prcp\", \"vp\", \"srad\"]].\\\n",
    "            assign(date = lambda x: x[\"date\"].dt.normalize())\n",
    "def trycatch_data(site):\n",
    "    try:\n",
    "        obs = nwis.get_dv(sites=[site], start=\"2015-01-01\", end=\"2022-12-31\", parameterCd=\"00010\")[0][\"00010_Mean\"].reset_index().rename(columns={\"datetime\": \"date\", \"00010_Mean\": \"temperature\"})\n",
    "        obs[\"date\"] = obs[\"date\"].dt.tz_localize(None).dt.normalize()\n",
    "        if len(obs) < 365:\n",
    "            print(\"\\nInsufficient data\", end=\"\")\n",
    "            return None\n",
    "        weather = get_weather(site)\n",
    "        (geom, lat, lon, area) = NEXT.data.gage_geom(site)\n",
    "        if area > 1e11:\n",
    "            print(\"\\nToo big\", end=\"\")\n",
    "            return None\n",
    "        statics = NEXT.data.geom_static_data(site, \"usgs\", geom, lat, lon, area)\n",
    "        covars = statics.merge(weather, how=\"cross\")\n",
    "        full = covars.merge(obs, on=\"date\", how=\"left\").dropna()\n",
    "        if len(full) >= 365:\n",
    "            print(\"|\", end=\"\")\n",
    "            return full\n",
    "        else:\n",
    "            print(\"\\nInsufficient data after merge\", end=\"\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\", end=\"\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75ca873b-3ae9-4d23-af3b-30c2b4cd3e67",
   "metadata": {
    "tags": []
   },
   "source": [
    "chunks = True\n",
    "chunks = bp + \"test_chunks/\"\n",
    "test_ids = [f.split(\".\")[0] for f in os.listdir(chunks)]\n",
    "existing = np.concatenate([dev_ids, test_ids])\n",
    "print(len(test_ids))\n",
    "if chunks:\n",
    "    test_gages = pd.read_csv(bp + \"GageData.csv\", dtype={\"id\": \"str\"})[\"id\"]\n",
    "    test_gages = test_gages[-test_gages.isin(existing)][175:]\n",
    "    print(len(test_gages))\n",
    "    for site in test_gages:\n",
    "        test_data = trycatch_data(site)\n",
    "        if test_data is not None:\n",
    "            test_data = test_data[(test_data[\"temperature\"] > -0.5) & (test_data[\"temperature\"] < 40)]\n",
    "            test_data.to_csv(chunks + site + \".csv\", index=False)\n",
    "pd.concat([pd.read_csv(chunks + f, dtype={\"id\": \"str\"}, parse_dates=[\"date\"]) for f in os.listdir(chunks)]).to_csv(bp + \"TestData.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b48dc9b3-0cc1-42fb-94f7-bdd4e213dab4",
   "metadata": {
    "tags": []
   },
   "source": [
    "buffers = pd.read_csv(bp + \"canopy.csv\", dtype={\"id\": \"str\"})\n",
    "wcan = [(fn, pd.read_csv(fn, dtype={\"id\": \"str\"}, parse_dates=[\"date\"]).assign(year=lambda x: x[\"date\"].dt.year).merge(buffers, on=[\"id\", \"year\"], how=\"left\"))\n",
    "        for fn in datafiles]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "709592b8-74e8-4c19-a8af-2b476df1f678",
   "metadata": {
    "tags": []
   },
   "source": [
    "for (fn, dat) in wcan:\n",
    "    dat.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2e368-0a05-4b30-bf90-2e5a059ebc24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Calibrated Tests\n",
    "\n",
    "Full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0945c-207d-4ecc-82f1-6f1907b4941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cut_dev(gid, data, start=None):\n",
    "    idd = data[data[\"id\"] == gid]\n",
    "    if idd[\"temperature\"].mean() > 35 or idd[\"temperature\"].mean() < 0:\n",
    "        return (None, None)  # bad data or major outlier\n",
    "    if start is None:\n",
    "        cut = round(len(idd) * 0.7)\n",
    "        if cut >= 365:\n",
    "            return (idd.iloc[cut:], idd[\"date\"].iloc[cut+1])\n",
    "        else:\n",
    "            return (None, None)  # dataset too small\n",
    "    else:\n",
    "        return idd[idd[\"date\"] >= start]\n",
    "\n",
    "def cal_val(gid, data, test_data = None, cal_fn = cut_dev, val_fn = cut_dev):\n",
    "    (train, cutoff) = cal_fn(gid, data)\n",
    "    if test_data is None:\n",
    "        test_data = data\n",
    "    # try:\n",
    "    if cutoff is not None:\n",
    "        test = cal_fn(gid, data, cutoff)\n",
    "        model = NEWT.Watershed.from_data(train)\n",
    "        if model is not None:\n",
    "            return model.run_series(test)\n",
    "    # except Exception as e:\n",
    "    #     warnings.warn(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ef967-0fc1-45fc-9232-5d2288be584e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hindcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c330f63-42d2-4542-be60-cf15fc0d2b98",
   "metadata": {},
   "source": [
    "In my last test, 919 watersheds took 26 minutes to train and predict, for about 2 seconds per watershed.\n",
    "\n",
    "With GAM-sensitivity, 1225 watersheds took 42 minutes to train and predict, still at ~2 seconds each.\n",
    "\n",
    "With full-series runs and GAM-sensitivity, 1225 watersheds took 10 minutes to train and predict, for ~0.5 seconds each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d58c13-562f-40cd-9bbe-ea65c3efe70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = bp + f\"results/calibrated_hindcast{newt_variant}.csv\"\n",
    "if not rerun and os.path.exists(file):\n",
    "    preds = pd.read_csv(file, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    # with warnings.catch_warnings(action=\"ignore\"):\n",
    "    preds = pd.concat([cal_val(x, all_data) for x in all_data[\"id\"].unique()])\n",
    "    runtime = (time.time() - start) / 60\n",
    "    count = len(preds[\"id\"].unique())\n",
    "    print(f\"Trained and ran {count} watershed models in {runtime:.1f} minutes.\")\n",
    "    preds.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724e66f-a97c-4823-9300-e15047a3a149",
   "metadata": {},
   "source": [
    "Overall performance summary below.  Note that stationarity, in particular, does absurdly well as a comparison point (same temperature today as yesterday).  As far as I'm aware, this comparison has not been run for most previous models.  It would be interesting to see how much of a lag is required for NEWT to outperform stationarity.  This does suggest that, if you have observations, \"same as yesterday\" is probably a better bet than most non-data-assimilating models.\n",
    "\n",
    "Interestingly, when a massive outlier that was predicting anomalies in the thousands of degrees is removed, global performance is very similar to gagewise performance.  Note that huge anomaly sensitivity isn't representative of any real use case, since in a calibrated model that would be corrected for and the (smoothed) coefficient estimation model won't predict such high sensitivity.  (If it did happen, it would be fairly obvious that ~3000 C is not a reasonable estimate.)\n",
    "\n",
    "Using GAM-anomaly reduced median and global RMSE by about 0.1 C to 1.4 C (for both).  Anomaly NSE increased to 0.61 (0.92 globally), median R2 to 0.95, and global R2 to 0.97.  Overall NSE increased to 0.95 and 0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc610d-303b-4646-ad3a-5a9605097e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    print(preds.groupby(\"id\").apply(NEWT.analysis.perf_summary).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db6a56-de5a-4277-bea5-2f85140fd2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEWT.analysis.perf_summary(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c39592-758d-43b8-b7de-8f4788a55234",
   "metadata": {},
   "source": [
    "## Forecast\n",
    "\n",
    "Trained and tested with HRRR.  Note this limits the data coverage (~2018-22).  Since the POR is shorter and there are fewer applicable watersheds, this takes 4 minutes.\n",
    "\n",
    "Performance here is very similar, though a little worse; at calibrated sites, the seasonal skill dominates and the slight penalty to anomaly performance has little effect.  Median/global R2 0.95/0.96, RMSE 1.4/1.5 C, NSE 0.95/0.96, pbias 0.007%/0.01%, max-miss 1.3/1.1 C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7df6a8-e389-4f19-a56a-e9f350638de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = bp + f\"results/calibrated_reforecast{newt_variant}.csv\"\n",
    "if not rerun and os.path.exists(file):\n",
    "    preds = pd.read_csv(file, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    # with warnings.catch_warnings(action=\"ignore\"):\n",
    "    preds = pd.concat([cal_val(x, all_data_hrrr) for x in all_data_hrrr[\"id\"].unique()])\n",
    "    runtime = (time.time() - start) / 60\n",
    "    count = len(preds[\"id\"].unique())\n",
    "    print(f\"Trained and ran {count} watershed models in {runtime:.1f} minutes.\")\n",
    "    preds.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038e867-e848-4914-83c7-599af718c346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    print(preds.groupby(\"id\").apply(NEWT.analysis.perf_summary).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1673e-fb86-487e-b425-2bd4c514bfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEWT.analysis.perf_summary(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd8d82-346b-4167-a08b-d8facd4761e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gagewise Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6b777-a199-48d6-aa83-957fbafda060",
   "metadata": {},
   "source": [
    "Runtime: 1.9 hours for 930 sites and ~3M observations.  That makes >1.5 years/second, assuming most of the time is prediction, and a total of 7.5 seconds per site.\n",
    "\n",
    "With no modification engines, it is 0.43 hours, making 1.7 seconds/site and >5 years/second.\n",
    "\n",
    "With full-series runs and GAM-sensitivity, training still takes a few minutes, but prediction is almost instantaneous.  It still takes 0.4 hours total, suggesting that runtime is dominated by model training.  Oddly, performance is not improved by the GAM-sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e73160-3101-4586-a0b3-16db7b0a4949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modbuilder: data -> (ws -> prediction)\n",
    "logfile = bp + \"results/kfold_log.txt\"\n",
    "def make_modbuilder(use_clim, lookback):\n",
    "    def next_modbuilder(data):\n",
    "        nx = NEXT.NEXT.from_data(data)\n",
    "        def prd(x):\n",
    "            print(\"|\", end=\"\")\n",
    "            # return nx.run(x, reset=True, use_climate=use_clim, climyears=lookback)\n",
    "            try:\n",
    "                # return nx.make_newt(x, reset=True, use_climate=use_clim, climyears=lookback).get_newt().coefs_to_df()\n",
    "                return nx.run(x, reset=True, use_climate=use_clim, climyears=lookback)\n",
    "                # mod = nx.make_newt(x, reset=True, use_climate=use_clim, climyears=lookback).get_newt()\n",
    "                # mod.dynamic_engine = None\n",
    "                # return mod.run_series(x)\n",
    "            except KeyboardInterrupt as e:\n",
    "                raise e\n",
    "            except Exception as e:\n",
    "                print(\"Error\", end=\"\")\n",
    "                with open(logfile, \"a\") as lf:\n",
    "                    lf.write(f\"Error in gage {x[\"id\"].iloc[0]} for use_clim={use_clim} and lookback={lookback}: {e}\\n\")\n",
    "                return None\n",
    "        return prd\n",
    "    return next_modbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb06b4-b5cf-49ab-a7ac-cb16ec1127ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clim = False\n",
    "lookback = 10\n",
    "timing = bp + \"results/kfold_times.txt\"\n",
    "out = bp + f\"results/kfold_pca{coef_variant}.csv\"\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    start = time.time()\n",
    "    kfr = NEWT.analysis.kfold(dev_data, make_modbuilder(clim, lookback), output=out, redo=rerun)\n",
    "    print(f\"\\nLookback: {lookback} : {(time.time() - start) / 3600: .2f} hours for {len(dev_data[\"id\"].unique())} sites with {len(dev_data)} observations\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb3d8a60-a940-4d1c-a8fa-df0eca90fe9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "kfr = pd.read_csv(bp + f\"results/kfold_pca{coef_variant}.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa72c25-27cc-441f-addb-7dc0fdbe1cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = kfr.groupby([\"id\", \"lat\", \"lon\"]).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index()\n",
    "perf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc368b7b-50f3-485a-8f05-818cd7c9f47b",
   "metadata": {},
   "source": [
    "FWPCA:\n",
    "\n",
    "- All PCs: R2 0.94, RMSE 2.1, NSE 0.90, anomaly 0.49, maxmiss 2.4, spatial predictability 0.78 (0.62-0.87) and 0.73 (0.60-0.82).\n",
    "- Five PCs: R2 0.94, RMSE 2.1, NSE 0.90, anomaly 0.56, maxmiss 2.0, spatial 0.78 (0.62-0.86) and 0.73 (0.61-0.82).\n",
    "- Two PCs: R2 0.93, RMSE 2.1, NSE 0.9, anomaly 0.56, maxmiss 2.0, spatial 0.79 (0.63-0.86) and 0.73 (0.61-0.81).\n",
    "- Three PCs, fixed dates: R2 0.94, RMSE 2.1, NSE 0.9, anomaly 0.57, maxmiss 2.0, spatial 0.78 (0.62-0.86) and 0.72 (0.61-0.82).\n",
    "- Four PCs, fixed dates: R2 0.94, RMSE 2.1, NSE 0.90, anomaly 0.57, maxmiss 2.1, spatial 0.79 (0.62-0.86) and 0.73 (0.61-0.82).\n",
    "- Six PCs, fixed dates: R2 0.94, RMSE 2.1, NSE 0.90, anomaly 0.56, maxmiss 2.1, spatial 0.79 (0.62-0.86) and 0.73 (0.61-0.83).\n",
    "- Two PCs, restricted variables: R2 0.93, RMSE 2.2, NSE 0.90, anomaly 0.56, maxmiss 2.1, spatial 0.76 (0.62-0.83) and 0.71 (0.60-0.79). Other performance is *atrocious*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d6054-6d75-45ad-90ed-45d0fc13d7bc",
   "metadata": {},
   "source": [
    "## Predictability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4141b45-79ae-492b-b396-a0bed9ed20e7",
   "metadata": {},
   "source": [
    "Analyze spatiotemporal predictability in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07be92-45a2-4922-b5a3-3e10274643e3",
   "metadata": {},
   "source": [
    "### Daily Spatial and Spatiotemporal\n",
    "\n",
    "Predictability of DOY-mean for each DOY.\n",
    "\n",
    "Predictability of daily/yearly timeseries for each DOY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adecc4-f634-43d5-87cb-a7d504229530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doymean = kfr.groupby([\"id\", \"day\"])[[\"temperature\", \"temp.mod\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f19d22-bc51-4457-84d4-b1d2be97e39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2s = doymean.groupby(\"day\").apply(lambda x: x[\"temperature\"].corr(x[\"temp.mod\"])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e0a81-1e81-47e3-9f41-b83b342dcc28",
   "metadata": {},
   "source": [
    "Spatial predictability is a median of 77%, ranging from 61-86%. So why the poor skill for coefficients? Maybe, while variance is more widely distributed, the *effect* is dominated by just a few..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fed5be-5dea-4fae-acd0-69ff1b29eefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4eb71f-b76c-4af9-9795-1b90e0da580d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2st = kfr.groupby(\"day\").apply(lambda x: x[\"temperature\"].corr(x[\"temp.mod\"])**2, include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9babc1-3c0a-43ff-8528-673c33d041c5",
   "metadata": {},
   "source": [
    "Spatiotemporal predictability is a little worse, ranging from 60-82% with a median of 72%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5496e4-ec20-4f65-849b-1d74c2d3f380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2st.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058233f-8cc0-413f-b628-bb82f9d2f856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(fig, [ax1, ax2]) = plt.subplots(1, 2, figsize=(5, 4), layout=\"compressed\", sharex=True, sharey=True)\n",
    "sns.kdeplot(r2s, ax=ax1)\n",
    "sns.kdeplot(r2st, ax=ax2)\n",
    "ax2.set_ylabel(None)\n",
    "ax1.set_ylabel(\"Probability Density\")\n",
    "ax1.set_xlabel(\"Spatial $R^2$ of Day-of-Year Means\")\n",
    "ax2.set_xlabel(\"Spatiotemporal $R^2$ by Julian day\")\n",
    "fig.supxlabel(\"Cross-Validation Predictability in Space\")\n",
    "plt.savefig(\"val_figures/spatial_skill_kde.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6c3d2-36f0-480a-969b-fc6b437c2668",
   "metadata": {},
   "source": [
    "### Seasonal Spatial\n",
    "\n",
    "Predictability of each 3S coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b8f6b-d1b9-48bb-b8fd-7e845fec864b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ssn(day, temps):\n",
    "    data = pd.DataFrame({\"day\": day, \"temperature\": temps})\n",
    "    try:\n",
    "        return ThreeSine.from_data(data, warn=False).to_df()\n",
    "    except:\n",
    "        return None  # failed fit\n",
    "ssn_obs = kfr.groupby(\"id\").apply(lambda x: get_ssn(x[\"day\"], x[\"temperature\"]), include_groups=False)\n",
    "ssn_mod = kfr.groupby(\"id\").apply(lambda x: get_ssn(x[\"day\"], x[\"temp.mod\"]), include_groups=False).rename(columns=lambda x: x + \"_mod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8e9f8-4096-4cd1-82a6-fb281410085f",
   "metadata": {},
   "source": [
    "The 2-PC model does underpredict both the SpringSummer and FallWinter coefficients, but still outperforms the full-PC model even for high elevations, evidently due to lower overall noise. The problem here is crappy predictability for snow components, evidently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee213b7-c4df-40d7-8614-46cd4c6cdf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_obs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3eeacb-c671-41a4-95f9-4e4f09b82116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_mod.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83c347-9848-4bb1-9d5a-93d0c3a5b162",
   "metadata": {},
   "source": [
    "Here, we see more expected behavior. Spatial skill is highly variable, ranging from just 3.4% (SummerDay) to 86% (Intercept). It's just that the low-skill coefficients don't contribute much; Intercept (86%) and Amplitude (56%) must dominate variability. We should also look at the NSE to see which ones are actually worth predicting.\n",
    "\n",
    "With the two-PCA, skill ranges from 1.9% (SummerDay) to 88% (Intercept). Mean and median are, unsurprisingly, lower, at 26% and 11%.  Three-PCA does substantially improve skill for some of the seasonal anomaly terms. Adding a fourth PC brings a further slight improvement to anomaly magnitude skill. Six-PCA makes further marginal improvements to SpringSummer and FallWinter, at the expense of SpringDay and SummerDay.  The original setup has far higher skill for FallWinter, in particular, and a little better in SpringSummer, despite somewhat worse overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa243a-b1d3-483b-afc2-40fe01755350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fits = pd.Series({\n",
    "    nm: ssn_obs[nm].corr(ssn_mod[nm + \"_mod\"])**2  # hand-verified they are all positive\n",
    "    for nm in ssn_obs.columns\n",
    "    if not nm in [\"R2\", \"RMSE\"]\n",
    "})\n",
    "fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d1e7f-8884-4e1b-a4f8-a84913071a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fits = pd.Series({\n",
    "    nm: ssn_obs[nm].corr(ssn_mod[nm + \"_mod\"])**2  # hand-verified they are all positive\n",
    "    for nm in ssn_obs.columns\n",
    "    if not nm in [\"R2\", \"RMSE\"]\n",
    "})\n",
    "fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6578f-323c-4f85-9877-f5a8b4aa07f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe09e5-3469-4f76-a5e9-9e6e963a6a58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3), layout=\"compressed\")\n",
    "sns.barplot(fits, ax=ax)\n",
    "ax.set_xlabel(\"Three-Sine Coefficient\")\n",
    "ax.set_ylabel(\"Cross-Validation $R^2$\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.savefig(\"val_figures/skill_3s.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44311ebb-769e-4e90-8567-301738c35bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nse(sim, obs):\n",
    "    mse = np.mean((sim - obs)**2)\n",
    "    ovar = obs.var()\n",
    "    return 1 - mse/ovar\n",
    "nses = pd.Series({\n",
    "    nm: nse(ssn_mod[nm + \"_mod\"], ssn_obs[nm])\n",
    "    for nm in ssn_obs.columns\n",
    "    if not nm in [\"R2\", \"RMSE\"]\n",
    "})\n",
    "nses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b4b74-1894-4163-b16b-9f8152f22933",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b66cb-be8f-43d6-a836-6817c92fadd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states = gpd.read_file(\"usa_states/cb_2018_us_state_20m.shp\")\n",
    "states = states[-states[\"STUSPS\"].isin([\"AK\", \"HI\", \"PR\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789f174-659e-4d69-b4ec-dd94d164077e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), layout=\"compressed\")\n",
    "ax = plt.axes(facecolor=\"#999\")\n",
    "states.plot(ax=ax, color=\"#333\")\n",
    "perf.plot.scatter(x=\"lon\", y=\"lat\", c=\"RMSE\", ax=ax, colormap=\"viridis\", vmin=0, vmax=5)\n",
    "cb = ax.collections[1].colorbar\n",
    "cb.set_label(\"Cross-Validation RMSE (C)\")\n",
    "ax.set_xlabel(\"Longitude (deg)\")\n",
    "ax.set_ylabel(\"Latitude (deg)\")\n",
    "plt.savefig(\"val_figures/conus_rmse.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c8d81-3ee0-48f5-af00-515057810209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), layout=\"compressed\")\n",
    "ax = plt.axes(facecolor=\"#999\")\n",
    "states.plot(ax=ax, color=\"#333\")\n",
    "perf.plot.scatter(x=\"lon\", y=\"lat\", c=\"Pbias\", ax=ax, colormap=\"bwr\", vmin=-100, vmax=100)\n",
    "cb = ax.collections[1].colorbar\n",
    "cb.set_label(\"Cross-Validation Bias (%)\")\n",
    "ax.set_xlabel(\"Longitude (deg)\")\n",
    "ax.set_ylabel(\"Latitude (deg)\")\n",
    "plt.savefig(\"val_figures/conus_bias.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcd137-b2c8-4eb6-87df-4bf91151b823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 2), layout=\"compressed\")\n",
    "ax = plt.axes()\n",
    "kfr[\"error\"] = kfr[\"temp.mod\"] - kfr[\"temperature\"]\n",
    "kfr[\"error\"].plot.hist(bins=100, ax=ax)\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_xlabel(\"Prediction Error (C)\")\n",
    "plt.savefig(\"val_figures/ErrorHist_xv.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c12142-bc2f-4c7e-99d4-4d300d5b8aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecos = gpd.read_file(\"ecoregions/NA_CEC_Eco_Level1.shp\").to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7cec35-2b00-4c60-a9a3-fb73173bcf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_name(df):\n",
    "    if len(df) > 0:\n",
    "        return df.iloc[0][\"NA_L1NAME\"].title()\n",
    "    else:\n",
    "        return \"NA\"\n",
    "perf_pts = gpd.GeoSeries([shapely.Point(x.lon, x.lat) for x in perf.itertuples()])\n",
    "pt_ecos = pd.Series([get_name(ecos[ecos.contains(x)]) for x in perf_pts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68da3f-87fc-4a01-8704-7027bb4085ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4), layout=\"compressed\")\n",
    "ax = plt.axes()\n",
    "perf[\"ecoregion\"] = [x.replace(\" \", \"\\n\") for x in pt_ecos]\n",
    "sns.boxplot(perf[perf[\"ecoregion\"] != \"NA\"], y=\"RMSE\", x=\"ecoregion\", ax=ax)\n",
    "plt.xticks(rotation=60)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_xlabel(\"EPA Level I Ecoregion\")\n",
    "ax.set_ylabel(\"Cross-Validation RMSE (C)\")\n",
    "plt.savefig(\"val_figures/EcoregionRMSEBox.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7587719-0432-4825-813a-1faa3a99b8e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perf[perf[\"RMSE\"] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deaef98-cc80-41c7-8cae-5c22342dc212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4), layout=\"compressed\")\n",
    "ax = plt.axes()\n",
    "perf[\"ecoregion\"] = [x.replace(\" \", \"\\n\") for x in pt_ecos]\n",
    "sns.boxplot(perf[perf[\"ecoregion\"] != \"NA\"], y=\"Pbias\", x=\"ecoregion\", ax=ax)\n",
    "plt.xticks(rotation=60)\n",
    "ax.set_ylim(-100, 100)\n",
    "ax.set_xlabel(\"EPA Level I Ecoregion\")\n",
    "ax.set_ylabel(\"Cross-Validation Bias (%)\\n(2 outliers exceed +100%)\")\n",
    "plt.savefig(\"val_figures/EcoregionBiasBox.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af4c87-9b15-42cb-be80-33294d9533c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test Set Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d2abb-24cd-4210-8c5c-c90f0c0932fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    model = NEXT.NEXT.from_data(dev_data)\n",
    "runtime = int(time.time() - start)\n",
    "print(f\"Took {runtime} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016dc3bb-77a9-4724-9d07-6b72c7b8e945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to_pickle(\"coefs.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db55ca7-2362-4648-b56e-ac592cb8c811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hindcast\n",
    "\n",
    "This (full timeseries for 750k rows/331 watersheds) took 22 minutes, 1300 seconds.  As before, that's about 1.5 years per second.  The next run took 1200 seconds.\n",
    "\n",
    "With single-pass and GAM-sensitivity, it took 24 seconds, or 86 years per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035218a-35ca-46dc-9a64-cc9f302a9a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(ws):\n",
    "    print(\"|\", end=\"\")\n",
    "    try:\n",
    "        return model.run(ws, reset=True)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883db116-e96c-4d0b-9465-ee72072ce700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppath = bp + f\"results/TestSet_hindcast{coef_variant}.csv\"\n",
    "if not rerun and os.path.exists(ppath):\n",
    "    preds = pd.read_csv(ppath, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        preds = test_data.groupby(\"id\").apply(predict, include_groups=False).reset_index().drop(columns=\"level_1\")\n",
    "    runtime = int(time.time() - start)\n",
    "    print(f\"Took {runtime} seconds to predict\")\n",
    "    preds.to_csv(ppath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d2915-c093-4e1c-854a-52ab5f4520c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = preds.groupby([\"id\", \"lat\", \"lon\"]).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index()\n",
    "perf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ebc81-9334-4e16-a560-39126ae7daf7",
   "metadata": {},
   "source": [
    "### Good/Bad Sites\n",
    "\n",
    "This allows you to look at the best- and worst-performing test set sites on a map and cluster analysis and see if anything is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037df052-994a-4e6b-b0d2-ff68cd4d1c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranked = perf.sort_values(\"RMSE\")\n",
    "best = ranked.iloc[:100].assign(title = [\"best\" + str(ix) for ix in range(1,101)])\n",
    "worst = ranked.iloc[-100:].assign(title = [\"worst\" + str(ix) for ix in range(100, 0, -1)])\n",
    "midpt = int(len(ranked) / 2)\n",
    "mid = ranked.iloc[(midpt-50):(midpt+50)].assign(title = [\"middle\" + str(ix) for ix in range(1, 101)])\n",
    "comb = pd.concat([best, worst, mid])[[\"lat\", \"lon\", \"title\"]]\n",
    "comb[\"id\"] = comb[\"title\"]\n",
    "pts = [shapely.Point(x.lon, x.lat) for x in comb.itertuples()]\n",
    "comb = gpd.GeoDataFrame(data=comb, geometry=pts, crs=4326)\n",
    "# comb.to_file(\"goodbadsites.json\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac7748-138a-4106-b3d4-331c40134707",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Quantifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2ad4c-4e06-48fc-88c5-50f59e28c661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41d1c3-c79a-4fce-ab2c-62e409307ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "worst.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b0d89-8c25-4fa0-bf1d-de2a89788570",
   "metadata": {},
   "source": [
    "#### Classification Analysis\n",
    "\n",
    "We'll use scikit-learn's Random Forest ensemble, since that's both simple and flexible.  70/30 train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b934cd-ff07-4f2c-a9b5-17b9326a65fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allbw = pd.DataFrame({\"id\": pd.concat([best[\"id\"], mid[\"id\"], worst[\"id\"]]), \"rank\": [\"best\"] * 100 + [\"mid\"] * 100 + [\"worst\"] * 100}\n",
    "                    ).merge(NEXT.coef_est.preprocess(test_data), on=\"id\"\n",
    "                           ).drop(columns=[\"level_1_x\", \"level_1_y\", \"date\", \"day\"])\n",
    "train = allbw.groupby(\"rank\").sample(n=70)\n",
    "test = allbw.loc[-allbw[\"id\"].isin(train[\"id\"]),:]\n",
    "getX = lambda x: x.drop(columns=[\"id\", \"rank\"])\n",
    "getY = lambda x: x[\"rank\"]\n",
    "# test = test.loc[test[\"rank\"] != \"mid\",:]\n",
    "# train = train.loc[train[\"rank\"] != \"mid\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80157761-868d-4e0e-9b56-5a4e82604ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(getX(train), getY(train))\n",
    "rf.score(getX(train), getY(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00b287-8eb1-4a1a-b7b1-db7593d394a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf.score(getX(test), getY(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71a891-8436-4a53-877a-0b984c453203",
   "metadata": {},
   "source": [
    "Accuracy is 38% (correctly classified).  Proportion correctly classified by chance should be 1/3. This accuracy is worse than it was with the all-PC model, so dropping PCs increases the randomness of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28983ec4-8d52-48d3-99b4-7acff4639f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reforecast (heterogeneous)\n",
    "\n",
    "Note that the time period is not the same as the hindcast test, due to limited HRRR archive coverage.\n",
    "\n",
    "Data retrieval uses the *earliest* HRRR run covering a given date (i.e., 48 hours out), so this is a 2-day forecast lead time (it uses this morning's forecast - where today's max temperature is itself one day of forecast - to predict tomorrow).\n",
    "\n",
    "For comparison purposes here, the median R2 of pure climatology (i.e., fitted seasonality) is 0.90.  Stationarity probably does better than NEXT, but stationarity isn't actually available in an ungaged context - though, of course, neither is climatology!\n",
    "\n",
    "RMSE did increase a fair bit, to 2.9 C.  That seems to be driven by an increase in bias (to 8%/1.0 C), which probably has to do with the heterogeneous training data.  (Next up: pull HRRR for the full dataset and do a homogeneous run.)  Still, the R2 is better than climatology at 0.92.  (See below: it's the heterogeneity.)  The PCA, GAM-sensitivity version did a bit better than the original here.\n",
    "\n",
    "2-PC version has a much lower penalty here, with an RMSE of 2.6 C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1857cf5-4276-49cf-9c44-7e7e98b968dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(ws):\n",
    "    print(\"|\", end=\"\")\n",
    "    try:\n",
    "        return model.run(ws, reset=True)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394f436-72c9-44bf-bc26-e6a8ed655a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppath = bp + f\"results/TestSet_reforecast{coef_variant}.csv\"\n",
    "if not rerun and os.path.exists(ppath):\n",
    "    preds = pd.read_csv(ppath, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        preds = test_data_hrrr.groupby(\"id\").apply(predict, include_groups=False).reset_index().drop(columns=\"level_1\")\n",
    "    runtime = int(time.time() - start)\n",
    "    print(f\"Took {runtime} seconds to predict\")\n",
    "    preds.to_csv(ppath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c62d55-d905-4379-b9bd-eb833c1364e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = preds.groupby([\"id\", \"lat\", \"lon\"]).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index()\n",
    "perf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103df94-bdee-49da-bc40-00cb825f675b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reforecast (homogeneous)\n",
    "\n",
    "Note that the time period is not the same as the hindcast test, due to limited HRRR archive coverage.\n",
    "\n",
    "Data retrieval uses the *earliest* HRRR run covering a given date (i.e., 48 hours out), so this is a 2-day forecast lead time (it uses this morning's forecast - where today's max temperature is itself one day of forecast - to predict tomorrow).\n",
    "\n",
    "For comparison purposes here, the median R2 of pure climatology (i.e., fitted seasonality) is 0.90.  Stationarity probably does better than NEXT, but stationarity isn't actually available in an ungaged context - though, of course, neither is climatology!\n",
    "\n",
    "It does better with a homogeneous reforecast; RMSE decreases to 2.3 C.  R2 doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a277e9-23f1-4a38-a077-0beacf6080af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    model = NEXT.NEXT.from_data(dev_data_hrrr.dropna())\n",
    "runtime = int(time.time() - start)\n",
    "print(f\"Took {runtime} seconds to train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c7066-2d9d-4ef3-9e0a-470ec5d5e0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to_pickle(\"coefs_hrrr.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07da648-acea-43dd-8a03-b5126020a374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(ws):\n",
    "    print(\"|\", end=\"\")\n",
    "    return model.run(ws, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94d7d-1653-40dd-8294-8f58931fcf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppath = bp + f\"results/TestSet_reforecast_homog{coef_variant}.csv\"\n",
    "if not rerun and os.path.exists(ppath):\n",
    "    preds = pd.read_csv(ppath, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        preds = test_data_hrrr.groupby(\"id\").apply(predict, include_groups=False).reset_index().drop(columns=\"level_1\")\n",
    "    runtime = int(time.time() - start)\n",
    "    print(f\"Took {runtime} seconds to predict\")\n",
    "    preds.to_csv(ppath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275de5d-552c-4743-b4b8-7a4b4bc6fbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = preds.groupby([\"id\", \"lat\", \"lon\"]).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index()\n",
    "perf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b0219-f10a-4744-9e63-cd87640114a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825173b-1da0-48c0-a7ef-cef280e9b695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(ws, Nco=100, Nanom=10):\n",
    "    print(\"|\", end=\"\")\n",
    "    try:\n",
    "        out = None\n",
    "        for i in range(Nco):\n",
    "            res = model.run(ws, draw=True, reset=True, quantiles=Nanom)\n",
    "            minires = res.loc[:, res.columns.str.startswith(\"temp.mod_\")]\n",
    "            minires.columns = [x + f\"_{i}\" for x in minires.columns]\n",
    "            if out is None:\n",
    "                out = res[[\"date\", \"temperature\"]]\n",
    "            out = pd.concat([out, minires], axis=1)\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad14d3-e9ec-44a1-b766-2f329408922a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppath = bp + f\"results/TestSet_hindcast_uncertainty{coef_variant}.csv\"\n",
    "if not rerun and os.path.exists(ppath):\n",
    "    preds = pd.read_csv(ppath, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "else:\n",
    "    start = time.time()\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        preds = test_data.groupby(\"id\").apply(predict, include_groups=False\n",
    "                                             ).reset_index().drop(columns=\"level_1\")\n",
    "    runtime = int(time.time() - start)\n",
    "    print(f\"Took {runtime} seconds to predict\")\n",
    "    preds.to_csv(ppath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461b8e9-450e-4937-98eb-289906b0ed9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = pd.concat([preds[[\"id\", \"date\", \"temperature\"]], preds.loc[:, preds.columns.str.startswith(\"temp.mod_\")]], axis=1)\n",
    "def brier(site_preds):\n",
    "    array = xr.DataArray(\n",
    "        site_preds.drop(columns=[\"date\", \"temperature\"]),\n",
    "        dims=[\"date\", \"ensemble\"],\n",
    "        coords={\"ensemble\": range(len(site_preds.columns)-2), \"date\": site_preds[\"date\"]}\n",
    "    )\n",
    "    obs = xr.DataArray(site_preds[\"temperature\"],\n",
    "                       dims=[\"date\"],\n",
    "                       coords={\"date\": site_preds[\"date\"]})\n",
    "    minv = ceil(obs.min())\n",
    "    maxv = floor(obs.max())\n",
    "    if maxv > minv:\n",
    "        thres = np.arange(minv, maxv)\n",
    "    else:\n",
    "        thres = [minv]\n",
    "    return brier_score_for_ensemble(array,\n",
    "                                    obs,\n",
    "                                    event_thresholds=thres,\n",
    "                                    ensemble_member_dim=\"ensemble\").to_pandas()\n",
    "scores = pd.DataFrame(values.groupby(\"id\").apply(brier, include_groups=False).rename(\"Score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f101d-a52e-411e-ab00-6c199fe7d536",
   "metadata": {},
   "source": [
    "Brier scores here are generally quite good - a perfect score is 0 (worst is 1), and even with just the anomaly ensemble, the vast majority of scores are <0.1.  A ten-member x ten-member ensemble gives a median of 0.056 and 75th percentile of 0.086, compared to 0.066 and 0.098 with just anomaly ensemble.  Global performance is similar, but excessively demanding with a large-scale run, and so not run here.  Scores are nearly constant at ~0.06 across midrange thresholds (5-25 C), and better near the extremes.  That's the MSE of 0/1 (exceeds/not exceeds), so it's on the wrong side 6% of the time.  Performance is almost identical with a full-sized ensemble.\n",
    "\n",
    "With the adjusted ensemble approach (including noise), the median score drops to ~0.05, with a maximum median across thresholds of ~0.06. Similar for 2-PC. However, 2-PC develops a trend of worse performance at higher thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff7dde-c49e-443c-a9a8-36a16b3d049d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dd844-00c0-4d75-8438-594605c4eb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores.groupby(\"threshold\").median().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2bd21-04fe-43e1-9078-47053a741d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.regplot(scores.reset_index(), x=\"threshold\", y=\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc68d0-4d1d-4712-aba6-d6c5e635994a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 3))\n",
    "sns.barplot(scores.groupby(\"threshold\").median(), x=\"threshold\", y=\"Score\", ax=ax)\n",
    "ax.set_xlabel(\"SWT Threshold (C)\")\n",
    "ax.set_ylabel(\"Brier Score\\n(Proportion Incorrect)\")\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc802770-b8b4-4788-9c77-14dd4b5c4479",
   "metadata": {},
   "source": [
    "### Interval Width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35322a8-947f-4465-9096-108e32e4358b",
   "metadata": {},
   "source": [
    "A version that's weighted towards anomaly does very poorly for 95% CI accuracy (i.e., not many observations tend to be within the CI) because uncertainty is dominated by coefficient estimation.\n",
    "\n",
    "With noise, the median width increases to 12 C (IQR: 10.7-13.5 C; total range: 2.4-21 C), and typical capture is 97% (IQR: 97-99.9%). Fewer than 95% of values are in the estimated IQR for just 18% of sites. It is now overly conservative, presumably because this adds error (noise + uncertainty) to uncertainty, thus double-counting uncertainty.\n",
    "\n",
    "At 50% confidence (median width 4.2 C), it's very conservative, with median 65% of observations in the 50% CI (21% of gages short of 50%).\n",
    "\n",
    "With 2-PC, it's a little insufficiently conservative. The 50% interval (2.4 C wide) captures 37%. Calibrating it wider by 1.5 brings 95% interval capture up to 97%, and the 50% CI is spot-on at 51%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1332c1e-ddc3-458b-a2bc-74a8fda9ff11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 0.5\n",
    "widths = values.loc[:, [\"id\", \"date\"]]\n",
    "widths[\"upper\"] = values.iloc[:, 3:].apply(lambda x: np.quantile(x, (1+size)/2), axis=1)\n",
    "widths[\"lower\"] = values.iloc[:, 3:].apply(lambda x: np.quantile(x, (1-size)/2), axis=1)\n",
    "widths[\"median\"] = values.iloc[:, 3:].apply(lambda x: np.quantile(x, 0.5), axis=1)\n",
    "widths[\"width\"] = widths[\"upper\"] - widths[\"lower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6bea-5683-434b-b7a1-5cee2558960e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ed0b9-99dd-4ed7-9035-c731059f5119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths[\"obs\"] = values[\"temperature\"]\n",
    "widths[\"isin\"] = (widths[\"obs\"] <= widths[\"upper\"]) & (widths[\"obs\"] >= widths[\"lower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a960125-fd5f-40b8-83a3-f6b8a8e1627e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths.groupby(\"id\")[\"isin\"].mean().describe()\n",
    "# widths[\"isin\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f0a25-4ce0-4dde-8f0a-21a7d8b0d5c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(widths.groupby(\"id\")[\"isin\"].mean() < size).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d650cc-e115-4add-a1b4-64f01e448245",
   "metadata": {},
   "source": [
    "### Example Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed54a0-e7ab-4175-bf50-db9173c1a598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "which_sites = pd.Series(values[\"id\"].unique()).sample(n=8)\n",
    "sample = values.loc[values[\"id\"].isin(which_sites)].melt([\"id\", \"date\", \"temperature\"])\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8922e-ecc6-4312-a4b5-c58f9f2aff1c",
   "metadata": {},
   "source": [
    "Excellent! With the new (overly-conservative) version, we do see wide ranges. We also see the occasional \"stray\" ensemble member, as one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d8801-cd98-464c-836c-e21dfb5d658d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.relplot(sample,\n",
    "            x=\"date\",\n",
    "            y=\"value\",\n",
    "            col=\"id\",\n",
    "            col_wrap=2,\n",
    "            height=1.5,\n",
    "            aspect=2,\n",
    "            kind=\"line\",\n",
    "            facet_kws={\"sharex\": False},\n",
    "            errorbar='pi').tick_params(axis='x', rotation=45)\n",
    "g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(\"\")\n",
    "g.fig.supxlabel(\"Date\")\n",
    "g.fig.supylabel(\"Daily Mean SWT (C) - Ensemble Mean and 95% Range\")\n",
    "g.tight_layout()\n",
    "g.savefig(\"val_figures/ensemble_plots.png\", dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06868ceb-75a9-4800-b71f-da5c6102d2e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extrapolation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0540cd1-821b-4f59-a3a9-6220d986b01b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regional\n",
    "\n",
    "For regional extrapolation, we split the CONUS into 16 grid cells (four-by-four), of which 15 actually contain gages and 14 have more than 1 gage.  Across those 14, we run a leave-one-out cross-validation, training a model on the other 13 and predicting all gages in the left-out grid cell.  This tests the ability of the model to make predictions for contiguous regions with no gages, meaning that it has no training data from nearby sites and has to go off its knowledge of general trends.\n",
    "\n",
    "The cell 0x0 (southwesternmost) only has one gage in it, so results from that one should probably be ignored, but we'll include it.  The next fewest gages is 9, in Maine (3x3).  Some cells are, of course, considerably sparser than others, and gage count additionally varies with the size of cells on the US borders, as in the case of both 0x0 and 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3cb90-c3db-4887-bb4d-9694401dc78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "latmin = all_data[\"lat\"].min()\n",
    "latmax = all_data[\"lat\"].max() + 1\n",
    "lonmin = all_data[\"lon\"].min()\n",
    "lonmax = all_data[\"lon\"].max() + 1\n",
    "latstep = (latmax - latmin)/N\n",
    "lonstep = (lonmax - lonmin)/N\n",
    "\n",
    "all_data[\"lat_cell\"] = ((all_data[\"lat\"] - latmin) / latstep).astype(\"int\")\n",
    "all_data[\"lon_cell\"] = ((all_data[\"lon\"] - lonmin) / lonstep).astype(\"int\")\n",
    "all_data[\"cell\"] = all_data[\"lon_cell\"].astype(\"str\") + \"x\" + all_data[\"lat_cell\"].astype(\"str\")\n",
    "sns.scatterplot(all_data.groupby(\"id\")[[\"lat\", \"lon\", \"cell\"]].first(), x=\"lon\", y=\"lat\", hue=\"cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849f0e8-e7a5-4d01-ad3e-4a168a77d41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modbuilder(data):\n",
    "    nx = NEXT.NEXT.from_data(data)\n",
    "    print(\"New region\", end=\"\")\n",
    "    def prd(x):\n",
    "        print(\"|\", end=\"\")\n",
    "        try:\n",
    "            return nx.run(x, reset=True)\n",
    "        except KeyboardInterrupt as e:\n",
    "            raise e\n",
    "        except:\n",
    "            return None\n",
    "    return prd\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    spatxv = NEWT.analysis.kfold(all_data, modbuilder, by='cell', k=1, output = bp + f\"results/SpatialXV{coef_variant}.csv\", redo=rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b134b-f95e-4348-aa7d-9a4d98b96839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    spatperf = spatxv.groupby([\"id\", \"cell\"]).apply(NEWT.analysis.perf_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2780a-d4b1-433a-9bd0-8e8eea561217",
   "metadata": {},
   "source": [
    "Globally, there is a negligible penalty, with RMSE at 2.3 C and R2 at 0.93.\n",
    "\n",
    "With 2-PC, the penalty is the same, with RMSE of 2.2 C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbda16-254e-4b18-ade8-43e33cca4ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatperf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb19cf7-5331-477a-8c22-4751cfc02195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatperf.groupby(\"cell\")[\"RMSE\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3a13e-e4ea-447a-9cfc-80fdea19cc92",
   "metadata": {},
   "source": [
    "Performance does vary somewhat with region, but all have a median RMSE of 2.0-3.0 C except the one-gage one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f94399-bbef-4f23-b8ce-9b1e275afc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(spatperf, x=\"cell\", y=\"RMSE\")\n",
    "ax.set_xlabel(\"Spatial Cell\")\n",
    "ax.set_ylabel(\"Gage RMSE (C)\\n(3 outliers exceed 10 C)\")\n",
    "ax.set_ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd53755-41a6-458c-b9a1-dfa73647b1d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Elevation\n",
    "\n",
    "For elevation extrapolation, we train on the lower 95% and test on the upper 5%.  We also just plot errors by elevation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c6be7-fecc-440c-9f71-cf645823b2e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Non-Extrapolating\n",
    "\n",
    "To look at bias, remove the `.abs()`.  There is no trend in bias and a modest trend in absolute error (about 2 -> 2.6 C in the combined cross-validation and test sets).  However, this seems to be because median errors are very high in the 2000-2500 m range; otherwise, there's no obvious trend.  With that group removed, the trend is present but weaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c1bec8-c64e-45d1-8843-6dc6051353b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv(bp + f\"results/TestSet_hindcast{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "kfr = pd.read_csv(bp + f\"results/kfold_pca{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "cols = [\"id\", \"elev_min\", \"date\", \"temp.mod\", \"temperature\"]\n",
    "preds = pd.concat([preds[cols], kfr[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d2e7b-f21f-437a-9685-0efd5e579f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds[\"error\"] = (preds[\"temp.mod\"] - preds[\"temperature\"]).abs()\n",
    "elev_perf = preds.groupby(\"id\")[[\"elev_min\", \"error\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1befc-49e1-4184-b321-1e7af836a3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lmplot(elev_perf, x=\"elev_min\", y=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51f4ec-472c-43c7-871a-15cc030c84ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elev_perf.assign(elev_bucket = lambda x: x[\"elev_min\"] // 500).groupby(\"elev_bucket\").median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b3fcc2-811b-468a-b20b-5380d4c71747",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extrapolating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbda8c2-1dce-4abd-b31b-6a24858bbb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data[\"elev_min\"].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec91874-b14d-49b5-ac32-2bfcc5d5662a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = all_data[all_data[\"elev_min\"] < 1800]\n",
    "test = all_data[all_data[\"elev_min\"] >= 1800].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9286e58-ff22-4cc8-a456-b56fa5856789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test[\"id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddb6f1-51c3-4c05-8b00-65acc928b1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, ws):\n",
    "    print(\"|\", end=\"\")\n",
    "    try:\n",
    "        return model.run(ws, reset=True)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    model = NEXT.NEXT.from_data(train)\n",
    "    prd = test.groupby(\"id\").apply(lambda x: predict(model, x), include_groups=False).reset_index().drop(columns=\"level_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7c257-3298-4bfc-ac88-acff122d571e",
   "metadata": {},
   "source": [
    "Extrapolating to high elevation increases median RMSE to 2.8 C (+0.6), which is a much lower penalty than TempEst 2 (+1.1).  Also, bias is much less, but R2 is penalized more.\n",
    "\n",
    "So... using canopy density instead of forest LC, it suddenly does way better for the very highest elevations (RMSE 3-4 C, weaker trend).  Could the issue be (in large part) either that \"forest\" over 3000 m is often \"patchy trees\", or that said \"patchy trees\" weren't being counted as \"forest\"?\n",
    "\n",
    "Bafflingly, 2-PC dramatically reduces the penalty, with an RMSE of just 2.5 C (+0.4), and the highest-elevation sites have RMSE < 3 C. But now for some reason I'm getting an RMSE of 2.8 C again. What changed? Also NSE is still high, which is weird.\n",
    "\n",
    "Well, one thing that's happening is a marked tendency to completely miss the peaks - like, not even close. It may be that the high NSE/high RMSE pairs are occurring for sites with very high variation, which mutes the impact of RMSE on NSE. Adding the full suite of weighted PCs does a spectacularly awful job, though error isn't much worse. (With the peaks, it seems like it's hugely underestimating some combination of Amplitude and SpringSummer.)\n",
    "\n",
    "Okay, I am *so* confused now. Going back to the original PC suite (albeit with date fixing) gets RMSE down to 2.5 C, but NSE is *lower*, but the plots look better???\n",
    "\n",
    "Aaaand... now with the original PC suite, the RMSE is up to 2.9 C. Is this just random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9bedc-3171-42c4-8a24-33877dc4fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = prd.groupby([\"id\", \"lat\", \"lon\", \"elev_min\"]).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index().drop(columns=\"level_4\")\n",
    "perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46a94d-fde1-4668-96e5-b1fb76c1fd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.lmplot(perf, x=\"elev_min\", y=\"RMSE\")\n",
    "ax.set_axis_labels(\"Pour Point Elevation (m)\", \"RMSE (C)\")\n",
    "ax.set(ylim=(0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c178adc-2f39-4f0f-bc01-eb22d8e6dc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = prd.groupby(\"id\", as_index=False)[\"date\"].count()\n",
    "eligible = perf[(perf[\"id\"].isin(counts[(counts[\"date\"] >= 365*3)][\"id\"])) &\n",
    "               (perf[\"elev_min\"] >= 2400)].sort_values(\"NSE\")\n",
    "best = eligible[eligible[\"RMSE\"] < 4].iloc[-2:][\"id\"]  # want to make sure it's low absolute error, too\n",
    "worst = eligible.iloc[:2][\"id\"]\n",
    "rand = eligible[-(eligible[\"id\"].isin(pd.concat([best, worst])))].sample(n=4)[\"id\"]\n",
    "ids = [list(best), list(rand)[:2], list(rand)[2:], list(worst)]\n",
    "labels = [\"Best\", \"Random\", \"Random\", \"Worst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633090d8-7b98-4d4d-8ea9-625abc402bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6), layout=\"compressed\", sharex=False, sharey=True)\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        which = ids[j][i]\n",
    "        prow = eligible[eligible[\"id\"] == which].iloc[0].to_dict()\n",
    "        site = prd[prd[\"id\"] == which].sort_values(\"date\")\n",
    "        title = f\"{prow['id']} ({labels[j]}):\\nNSE={round(prow['NSE'], 2)}, RMSE={round(prow['RMSE'], 2)} C\"\n",
    "        ax.plot(site[\"date\"], site[\"temperature\"], label=\"Observed\")\n",
    "        ax.plot(site[\"date\"], site[\"temp.mod\"], label=\"TempEst-NEXT\")\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis='x', labelrotation=45)\n",
    "        if i==1 and j==2:  # hand-selected, retune as needed\n",
    "            ax.legend(loc=\"upper center\")\n",
    "fig.supxlabel(\"Date\")\n",
    "fig.supylabel(\"Daily Mean Stream Temperature (C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e93ce8-0bde-4561-b5cf-161089c3892f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ssn(day, temps):\n",
    "    data = pd.DataFrame({\"day\": day, \"temperature\": temps})\n",
    "    try:\n",
    "        return ThreeSine.from_data(data, warn=False).to_df()\n",
    "    except:\n",
    "        return None  # failed fit\n",
    "ssn_obs = prd.groupby(\"id\").apply(lambda x: get_ssn(x[\"day\"], x[\"temperature\"]), include_groups=False)\n",
    "ssn_mod = prd.groupby(\"id\").apply(lambda x: get_ssn(x[\"day\"], x[\"temp.mod\"]), include_groups=False).rename(columns=lambda x: x + \"_mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a1b62-77b0-434f-b08e-050304efc2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssn_obs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc163d9-d004-4d76-a9ee-b9bf04c71289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssn_mod.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042ec30-91dc-45c3-ac82-0ceee3afe2de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Walk-Forward\n",
    "\n",
    "For walk-forward validation, we train a model on all sites up to a given year, then predict the next five year.  This is done for all prediction years from 2010-2022, allowing ten years of initial training data.  We predict five years at a time to provide sufficient coefficient estimation data, since it's running in ungaged mode.\n",
    "\n",
    "RMSE goes up to 2.3 C again (2.2 C for 2-PC), with little penalty to R2 or bias.  So spatial and temporal extrapolation seem to have similar effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae78a19-9a83-4184-8389-9ea30e2ada81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data[\"year\"] = all_data[\"date\"].dt.year\n",
    "years = list(range(2010, 2023, 5))\n",
    "file = bp + f\"results/WalkForward{coef_variant}.csv\"\n",
    "def runmod(mod, data):\n",
    "    # Try/catch because some single-year snapshots don't have enough data.\n",
    "    # This shouldn't happen for prediction use, but as implemented for testing,\n",
    "    # missing ST obs = missing data, since the rows get dropped.\n",
    "    try:\n",
    "        return mod.run(data, reset=True, use_climate=False)\n",
    "    except:\n",
    "        return None\n",
    "def runner(year):\n",
    "    print(year)\n",
    "    model = NEXT.NEXT.from_data(all_data[all_data[\"year\"] < year])\n",
    "    inp = all_data[(all_data[\"year\"] >= year) & (all_data[\"year\"] < year + 5)]\n",
    "    return inp.groupby(\"id\").apply(lambda x: runmod(model, x), include_groups=False)\n",
    "if not rerun and os.path.exists(file):\n",
    "    wfv = pd.read_csv(file, dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "    wfv[\"id\"] = wfv.index\n",
    "else:\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        wfv = pd.concat([runner(year) for year in years])\n",
    "    wfv.to_csv(file, index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7ab54-c559-4d2d-96cd-c7a12865a3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = wfv.groupby(\"id\").apply(NEWT.analysis.perf_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5c0cc-ca15-40bd-848b-8c6e24b0e402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccede4-12ca-4252-b971-4f5cd2f61999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf_byyr = wfv.groupby([\"id\", \"year\"]).apply(NEWT.analysis.perf_summary).groupby(\"year\").median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e7ddf-ca18-46a4-ab9e-35dc629845fa",
   "metadata": {},
   "source": [
    "The first prediction years are 2010, 2015, and 2020.  These aren't distinctly low-error years, nor are the last (2014, 2019, 2022) consistently high-error.  So it doesn't seem to have any problem with extrapolating forward, in terms of RMSE.  We also don't see a trend in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cff99c-0253-4386-bb99-fe31b2d0ecd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.lineplot(perf_byyr, x=perf_byyr.index, y=\"RMSE\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Median RMSE (C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b359a3-b4b7-4ca8-bea0-4018a5abb65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perf_byyr[\"pyr\"] = list(range(1, 6))*2 + [1,2,3]\n",
    "ax = sns.regplot(perf_byyr, x=\"pyr\", y=\"RMSE\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Median RMSE (C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb72943-797e-4387-9113-02fa05f95ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(perf_byyr[\"pyr\"], perf_byyr[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28447fa-3c1e-4964-844d-30f5ed35de83",
   "metadata": {},
   "source": [
    "## Regime Shift/Disturbance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9044da-f998-4c51-a249-89127b458964",
   "metadata": {},
   "source": [
    "# Small Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e830c-c29d-4191-858a-5b95937cee5c",
   "metadata": {},
   "source": [
    "# True Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509470ec-8943-4d7f-8e0e-4aa34345e554",
   "metadata": {},
   "source": [
    "For forecast testing, we run a 16-day forecast using HRRR and GFS (HRRR for day 1-2, GFS after that).  We also use HRRR to fill in \"last week\" for anomaly smoothing.  Then, we come back in a few weeks and check the performance.  Performance testing should use RMSE and bias only, since trend metrics (R2, NSE) will have minimal observed variance to work with.  Performance should be analyzed aggregated by lead time.  For the 2-day lead (tomorrow's mean), we should expect the RMSE to converge on about 2.4 C.\n",
    "\n",
    "Since this is an ungaged model, and the only ungaged forecasting model, there is no reference performance to use for computing a skill score.  Instead, the question is whether the error is sufficiently low as to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f0fed-fea4-4aeb-b2a1-52e996a4a52a",
   "metadata": {},
   "source": [
    "## Shape Retrieval\n",
    "\n",
    "As of this writing, the NLDI service is down, so we're going to use cached shapefiles.  This is (hopefully) temporary and should be changed to use regular retrieval functions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f027d2d-04ac-45f6-9497-28e7f115ed01",
   "metadata": {
    "tags": []
   },
   "source": [
    "sites = list(test_data_hrrr[\"id\"].unique())\n",
    "filens = {s: f\"/scratch/dphilippus/AOI/USGS_{s}.shp\" for s in sites}\n",
    "shapes = {s: gpd.read_file(filens[s]) for s in filens if os.path.exists(filens[s])}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88170e89-519e-441c-8e62-8d7583bc0c56",
   "metadata": {
    "tags": []
   },
   "source": [
    "shp = shapes['01022500']\n",
    "dates = pd.Series(np.arange(np.datetime64(\"2025-02-01\"), np.datetime64(\"2025-02-08\"))).dt.strftime(\"%Y%m%d\")\n",
    "hs = wforecast.hrrr_series(shp, dates, \"TMP\", \"tmax\", lambda x: x.max())\n",
    "gs = wforecast.get_gfs_downloaded(shp, \"20250207\", bp + \"gfs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25f0baf-5de3-4546-9ae4-2d9e17ca3dcb",
   "metadata": {},
   "source": [
    "## Run Forecast"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49554e65-f2ac-4704-bb97-943520df06e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = NEXT.NEXT.from_pickle(\"coefs_hrrr.pickle\")\n",
    "today = np.datetime64(\"today\")\n",
    "start = today - np.timedelta64(7, \"D\")\n",
    "switch = today + np.timedelta64(2, \"D\")\n",
    "end = today + np.timedelta64(16, \"D\")\n",
    "datelist = lambda x, y: pd.Series(np.arange(x, y)).dt.strftime(\"%Y%m%d\")\n",
    "outf = bp + \"forecasts/\" + str(today) + \".csv\"\n",
    "stime = time.time()\n",
    "for gid in shapes:\n",
    "    coef_est = test_data_hrrr.loc[test_data_hrrr[\"id\"] == gid, :]\n",
    "    hrrr = wforecast.hrrr_series(shapes[gid], datelist(start, switch), \"TMP\", \"tmax\", lambda x: x.max())\n",
    "    gfs = wforecast.get_gfs_downloaded(shapes[gid], \"today\", bp + \"gfs/\")\n",
    "    ts = pd.concat([hrrr, gfs]).groupby(\"date\").agg(\"first\").reset_index()  # remove overlap\n",
    "    ts[\"id\"] = gid\n",
    "    ts = ts[[\"id\", \"date\", \"tmax\"]]  # id first\n",
    "    result = model.make_newt(coef_est, reset=True).run(ts)\n",
    "    result.to_csv(outf, index=False, header=not os.path.exists(outf), mode=\"a\")\n",
    "    print(\"|\", end=\"\")\n",
    "print(f\"\\nRuntime: {(time.time() - stime) / 60: .1f} minutes for {len(shapes)} sites with 24 days each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84c658-c643-4252-85d3-1b0adaa12a8b",
   "metadata": {},
   "source": [
    "## Verify Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525cc3b-0881-4023-8a3e-3d913f67eb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fcp = bp + \"forecasts/\"\n",
    "def parse_fc(fn):\n",
    "    rundate = np.datetime64(fn.split(\".\")[0])\n",
    "    fcdat = pd.read_csv(fcp + fn, parse_dates=[\"date\"], dtype={\"id\": \"str\"})\n",
    "    fcdat = fcdat.loc[fcdat[\"date\"] >= rundate, :]\n",
    "    fcdat[\"rundate\"] = rundate\n",
    "    fcdat[\"lead\"] = ((fcdat[\"date\"] - rundate + np.timedelta64(1, 'D'))/np.timedelta64(1, 'D')).astype(int)  # +1: today's mean is a 1-day lead.\n",
    "    return fcdat\n",
    "forecast = pd.concat([parse_fc(fn) for fn in os.listdir(fcp)])\n",
    "forecast = forecast.loc[forecast[\"lead\"] < 17]  # 17-day shouldn't be in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0717e6-eb1e-46eb-8382-2ba1373f5f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites = list(forecast[\"id\"].unique())\n",
    "# Run from first forecast date.\n",
    "obs = nwis.get_dv(sites=sites, start=\"2025-02-07\", end=\"2030-01-01\", parameterCd=\"00010\")[\n",
    "    0].reset_index()[[\"site_no\", \"datetime\", \"00010_Mean\"]].rename(\n",
    "        columns={\"00010_Mean\": \"temperature\", \"site_no\": \"id\", \"datetime\": \"date\"}\n",
    "    ).dropna().assign(date=lambda x: x[\"date\"].dt.normalize().dt.tz_localize(None))\n",
    "obs = obs[obs[\"temperature\"] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9084b-0d18-4494-ab8c-24f8204c6d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged = forecast.merge(obs, on=[\"id\", \"date\"], how=\"left\").dropna()\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = merged.groupby([\"id\", \"lead\"]).apply(NEWT.analysis.perf_summary, include_groups=False)\n",
    "perf[[\"RMSE\", \"Bias\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c860b-8123-405c-ba71-507ccdb81a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meds = perf.groupby(\"lead\")[[\"RMSE\", \"Bias\"]].median()\n",
    "meds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a32e2c-6aaf-4576-bb47-8bdc6890222f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bylead = merged.groupby(\"lead\").apply(NEWT.analysis.perf_summary, include_groups=False)\n",
    "bylead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af56dc-710a-48e9-aa19-b686090d4b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "round(meds.reset_index().merge(bylead, on=\"lead\", suffixes=[\"_median\", \"_global\"])[[\"lead\", \"RMSE_median\", \"RMSE_global\", \"Bias_median\", \"Bias_global\"]\n",
    "                                                                                  ].rename(\n",
    "    columns={\"lead\": \"Lead Time (days)\",\n",
    "             \"RMSE_median\": \"Median RMSE (C)\",\n",
    "             \"RMSE_global\": \"Global RMSE (C)\",\n",
    "             \"Bias_median\": \"Median Bias (C)\",\n",
    "             \"Bias_global\": \"Global Bias (C)\"}), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddd2d0-3a83-4769-8d06-7d705f82df1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEWT.analysis.perf_summary(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe0fd2-f46a-4b47-9c0e-3b6945bc227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ax = merged.plot.scatter(x=\"temp.mod\", y=\"temperature\")\n",
    "pmerged = merged.merge(bylead.round(2), on=\"lead\", how=\"left\")\n",
    "minmax = [merged[\"temperature\"].min(), merged[\"temperature\"].max()]\n",
    "minmax = pd.DataFrame({\"x\": minmax, \"y\": minmax})\n",
    "# pmerged[\"lead\"] = pmerged[\"lead\"].astype(str) + \" (RMSE: \" + pmerged[\"RMSE\"].astype(str) + \" C)\"\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5), layout=\"compressed\")\n",
    "# sns.scatterplot(pmerged, x=\"temperature\", y=\"temp.mod\", hue=\"lead\", ax=ax)\n",
    "fig = sns.relplot(pmerged, x=\"temperature\", y=\"temp.mod\", col=\"lead\", col_wrap=4,\n",
    "                   height=2, aspect=1, kind=\"scatter\")\n",
    "for ax in fig.axes:\n",
    "    sns.lineplot(minmax, x=\"x\", y=\"y\", ax=ax)\n",
    "fig.figure.supxlabel(\"Observed Temperature (C)\")\n",
    "fig.set_xlabels(\"\")\n",
    "fig.figure.supylabel(\"Modeled Temperature (C)\")\n",
    "fig.set_ylabels(\"\")\n",
    "fig.set_titles(col_template=\"Lead Time: {col_name} days\")\n",
    "# ax.plot(pmerged[\"temperature\"], pmerged[\"temperature\"], label=\"1:1\")\n",
    "# ax.set_ylabel(\"Modeled Temperature (C)\")\n",
    "# ax.set_xlabel(\"Observed Temperature (C)\")\n",
    "# ax.legend().set_title(\"Lead Time (days)\")\n",
    "plt.savefig(\"val_figures/forecast.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394cccc-ccbf-48f9-abc8-67b87bb67e50",
   "metadata": {},
   "source": [
    "## National Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71363a7d-5b50-4eaf-947b-02c4335e0814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states = gpd.read_file(\"usa_states/cb_2018_us_state_20m.shp\")\n",
    "states = states[-states[\"STUSPS\"].isin([\"AK\", \"HI\", \"PR\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605b0f9-a3b1-4842-9110-0944f5a8289b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = test_data_hrrr.groupby(\"id\")[[\"lat\", \"lon\"]].agg(\"first\")\n",
    "bestworst = (perf.groupby(\"id\").\n",
    "             apply(lambda x: pd.DataFrame({\"best\": x[\"RMSE\"].min(), \"worst\": x[\"RMSE\"].max()}, index=[0]),\n",
    "                   include_groups=False).\n",
    "             merge(coords, on=\"id\", how=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a014eb-7bbc-435a-bea5-64f38ebdf0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), layout=\"compressed\")\n",
    "ax = plt.axes(facecolor=\"#CCC\")\n",
    "states.plot(ax=ax, color=\"#999\")\n",
    "bestworst.plot.scatter(x=\"lon\", y=\"lat\", c=\"best\", ax=ax, colormap=\"viridis\", vmin=0, vmax=5)\n",
    "cb = ax.collections[1].colorbar\n",
    "cb.set_label(\"Forecast RMSE (C) - Lowest Over Lead Times\")\n",
    "ax.set_xlabel(\"Longitude (deg)\")\n",
    "ax.set_ylabel(\"Latitude (deg)\")\n",
    "plt.savefig(\"val_figures/conus_forecast_best.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5132487-c396-43bd-8fc9-bd9602b5529e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bestworst.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03440163-e0ed-490c-81e9-9130f748b8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4), layout=\"compressed\")\n",
    "ax = plt.axes(facecolor=\"#CCC\")\n",
    "states.plot(ax=ax, color=\"#999\")\n",
    "bestworst.plot.scatter(x=\"lon\", y=\"lat\", c=\"worst\", ax=ax, colormap=\"viridis\", vmin=0, vmax=5)\n",
    "cb = ax.collections[1].colorbar\n",
    "cb.set_label(\"Forecast RMSE (C) - Highest Over Lead Times\")\n",
    "ax.set_xlabel(\"Longitude (deg)\")\n",
    "ax.set_ylabel(\"Latitude (deg)\")\n",
    "plt.savefig(\"val_figures/conus_forecast_worst.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe788e-7b38-4707-a5dc-f4c18a2d27da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Error Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3826062-61ba-4f39-9fe6-7e39bda78dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv(bp + f\"results/TestSet_hindcast{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "kfr = pd.read_csv(bp + f\"results/kfold_pca{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "gcols = [\"id\", \"elev_min\", \"elev\", \"lat\", \"lon\", \"area\", 'water', 'developed', 'barren', 'forest', 'shrubland', 'herbaceous',\n",
    "       'cultivated', 'wetland', 'ice_snow', \"slope\", \"flowdir\"]\n",
    "ycols = [\"date\", \"temp.mod\", \"temperature\"]\n",
    "cols = gcols + ycols\n",
    "preds = pd.concat([preds[cols], kfr[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30d88a-a887-4bac-a678-e1fbc574580a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = np.arange(10, 0, -1)\n",
    "x[x<y] = y[x<y]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ffc91-3f63-4bd4-8f17-ae6f4e2bfaa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    perf = preds.groupby(gcols).apply(NEWT.analysis.perf_summary, include_groups=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a387e-cde7-4ee4-8f4a-eddbd1644a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hap = perf[perf[\"area\"] > 3e10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f6a14-0169-4be7-a948-1fe0a2a1c7a4",
   "metadata": {},
   "source": [
    "Look list (R>0.1 across pbias, R2, or NSE):\n",
    "\n",
    "- Water: effects negligible, it's noise because of few sites >5%.\n",
    "- Slope: there is definitely some performance penalty to steep watersheds, but minimal.\n",
    "- Latitude: is just because the warm Southeastern sites do well.\n",
    "- Shrubland: definitely correlates with bias, but weakly (R2 0.01).\n",
    "- Longitude: Western sites have a bit worse R2, which isn't news.\n",
    "- Forest: does correlate with some penalty to R2 (R2 0.04, p~0).\n",
    "- Cultivated: just noise from the few sites >20%.\n",
    "- Developed: same.\n",
    "- Barren: there's a real penalty to R2, but weak.\n",
    "- Wetland: nothing there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe06cb-197b-4a55-9c26-e52bf7e6201d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel = perf[perf[\"area\"] > 3e10]\n",
    "ax = sns.regplot(sel, x=\"area\", y=\"RMSE\")\n",
    "# ax.set_xlim(4e10, 10e10)\n",
    "# ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a166d-6657-47fd-abbf-86a871ade593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(sel[\"area\"], sel[\"Pbias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024eda0-229c-41e3-867b-f3f0542ee66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perf[gcols[1:]].agg(lambda x: scipy.stats.pearsonr(x, perf[\"NSE\"]).statistic).abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8e93d-6cf6-4aff-ae2a-3e4fe44ba327",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Misc Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67e56f-cc5d-40b6-906f-af321de8a025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv(bp + f\"results/TestSet_hindcast{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "# kfr = pd.read_csv(bp + f\"results/kfold_pca{coef_variant}.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])\n",
    "cols = [\"id\", \"date\", \"temp.mod\", \"temperature\"]\n",
    "# preds = pd.concat([preds[cols], kfr[cols]])\n",
    "# preds = pd.read_csv(bp + \"TE2Xval.csv\", dtype={\"id\": \"str\"}, parse_dates=[\"date\"])[cols]\n",
    "# preds = preds.loc[preds[\"temperature\"] < 35, :]  # for direct comparison to USGS dataset\n",
    "perf = preds.groupby(\"id\").apply(NEWT.analysis.perf_summary, include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc778a63-35ac-4141-bb3e-2ca772ff1dd6",
   "metadata": {},
   "source": [
    "## 1:1 Plot with Stats"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf671b7c-09d0-45eb-b2fe-14f522e8b961",
   "metadata": {
    "tags": []
   },
   "source": [
    "r2 = round(preds[\"temp.mod\"].corr(preds[\"temperature\"])**2, 3)\n",
    "rmse = round(np.sqrt(np.mean((preds[\"temp.mod\"] - preds[\"temperature\"])**2)), 3)\n",
    "label = f\"(10,000 random observations)\\nGlobal R2: {r2}\\nGlobal RMSE: {rmse} C\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4), layout=\"compressed\")\n",
    "sam = preds.sample(n=10000)\n",
    "sam.plot.scatter(x=\"temp.mod\", y=\"temperature\", ax=ax, alpha=0.5)\n",
    "ax.plot(sam[\"temperature\"], sam[\"temperature\"], label=\"1:1\", c=\"black\")\n",
    "ax.set_xlabel(\"Modeled Temperature (C)\")\n",
    "ax.set_ylabel(\"Observed Temperature (C)\")\n",
    "ax.text(20, 1, label)\n",
    "plt.savefig(\"val_figures/te2/one_one_plot.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94317f7f-87f4-4688-8ded-4afaae9fd9e1",
   "metadata": {},
   "source": [
    "## Sitewise R2 CDF"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7ed90ec-14d5-4686-8a67-f1395740e610",
   "metadata": {
    "tags": []
   },
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), layout=\"compressed\")\n",
    "sns.ecdfplot(perf, x=\"R2\", ax=ax)\n",
    "plt.savefig(\"val_figures/te2/r2_ecdf.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644c6ee-d333-4e74-a0a0-9cb00d555d52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Temperature Density"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5dc16a8-ad43-4321-ba8d-3378beeaebe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4), layout=\"compressed\")\n",
    "sam = preds.sample(n=100000)\n",
    "sam[\"temperature\"].plot.kde(label=\"Observed\", ax=ax, legend=True)\n",
    "sam[\"temp.mod\"].plot.kde(label=\"Modeled\", ax=ax, legend=True)\n",
    "ax.set_xlabel(\"Mean Daily Temperature (C)\")\n",
    "ax.set_xlim(-3, 40)\n",
    "plt.savefig(\"val_figures/te2/temp_density.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd002d2-6478-4bbc-bb4b-8c78b65cb4c7",
   "metadata": {},
   "source": [
    "## Sim/Obs Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac21aed-575c-4ede-b850-d2a969d8edb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = preds.groupby(\"id\", as_index=False)[\"date\"].count()\n",
    "eligible = perf.reset_index()[perf.unstack().index.isin(counts[(counts[\"date\"] >= 365*3)][\"id\"])].sort_values(\"R2\")\n",
    "best = eligible[eligible[\"RMSE\"] < 1.8].iloc[-2:][\"id\"]  # want to make sure it's low absolute error, too\n",
    "worst = eligible.iloc[:2][\"id\"]\n",
    "rand = eligible[-(eligible[\"id\"].isin(pd.concat([best, worst])))].sample(n=4)[\"id\"]\n",
    "ids = [list(best), list(rand)[:2], list(rand)[2:], list(worst)]\n",
    "labels = [\"Best\", \"Random\", \"Random\", \"Worst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ad4a6-5b46-4739-b133-76bf74702cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eligible.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821222b7-e37f-47d6-b1cd-45b1501b8bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6), layout=\"compressed\", sharex=False, sharey=True)\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        which = ids[j][i]\n",
    "        prow = eligible[eligible[\"id\"] == which].iloc[0].to_dict()\n",
    "        site = preds[preds[\"id\"] == which].sort_values(\"date\")\n",
    "        title = f\"{prow['id']} ({labels[j]}):\\nR2={round(prow['R2'], 2)}, RMSE={round(prow['RMSE'], 2)} C\"\n",
    "        ax.plot(site[\"date\"], site[\"temperature\"], label=\"Observed\")\n",
    "        ax.plot(site[\"date\"], site[\"temp.mod\"], label=\"TempEst-NEXT\")\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis='x', labelrotation=45)\n",
    "        if i==1 and j==2:  # hand-selected, retune as needed\n",
    "            ax.legend(loc=\"upper center\")\n",
    "fig.supxlabel(\"Date\")\n",
    "fig.supylabel(\"Daily Mean Stream Temperature (C)\")\n",
    "plt.savefig(\"val_figures/comparison_sample.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759ed92-2080-4f7b-9c52-497e4283e17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659846b-a072-48e5-9a3c-c0d43f89f35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "next",
   "language": "python",
   "name": "next"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
